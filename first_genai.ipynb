{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdKvLfsoo0s0vjeWVTWzR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopisai123/new_genai/blob/main/first_genai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk  #natular language tool kit library offers prebuilt algori , datasets,and lot of module as below etc\n",
        "# Major Components of NLTK\n",
        "# Module\tDescription\n",
        "# nltk.tokenize\tSplits text into sentences or words.\n",
        "# nltk.corpus\tAccess to large collections of text (like stopwords, movie reviews, names, etc.).\n",
        "# nltk.stem\tPerforms stemming (reduces words to their root form).\n",
        "# nltk.tag\tPart-of-speech tagging for words.\n",
        "# nltk.chunk\tNamed entity recognition and phrase chunking.\n",
        "# nltk.parse\tParsing text using grammar rules.\n",
        "# nltk.classify\tTools for text classification.\n",
        "# nltk.metrics\tFunctions to evaluate NLP models.\n",
        "# nltk.probability\tLanguage modeling, frequency distributions.\n",
        "# nltk.sem\tTools for representing and processing meaning (semantics)."
      ],
      "metadata": {
        "id": "SSXG4JIBKdg3",
        "outputId": "a290720f-8b58-4d4a-b035-f53b220f84dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn #library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a02qizXLDOGO",
        "outputId": "8074d962-ea5e-436c-b994-537345a7076d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 TOKENIZATION\n",
        "# \"I LIKE IBM SESSION\" -> [\"I\", \"LIKE\", \"IBM\", \"SESSION\"]\n",
        "# Each word is referred to as Tokens - 4\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+') #\\w+ wont considee spaces, punctution etc , as  it didn consider  fullstop while tokenizing\n",
        "\n",
        "text = \"I LIKE IBM SESSION!\"\n",
        "print(text.split())\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "print(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7TsLQTlFR0J",
        "outputId": "db877819-c9a9-445c-ba97-689ee026bcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'LIKE', 'IBM', 'SESSION!']\n",
            "I LIKE IBM SESSION!\n",
            "['I', 'LIKE', 'IBM', 'SESSION']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 stopwords removal\n",
        "\n",
        "# The purpose of this code is to remove stopwords from a list of tokens.\n",
        "# Stopwords are common words (like \"the\", \"is\", \"in\") that are usually filtered out in NLP tasks.\n",
        "\n",
        "# Example input:  [Yesterday I went to shopping inorder to buy an Iphone]\n",
        "# Expected output: [Yesterday, went, shopping, inorder, buy, Iphone]\n",
        "\n",
        "from nltk.corpus import stopwords  # Import the stopwords list from NLTK\n",
        "import nltk  # Import the NLTK library\n",
        "\n",
        "nltk.download('stopwords')  # Download the stopwords data (only needs to be done once)\n",
        "\n",
        "tokens = ['I', 'LIKE', 'IBM', 'SESSION']  # List of words (tokens) to process\n",
        "stop_words = set(stopwords.words('english'))  # Create a set of English stopwords\n",
        "\n",
        "# List comprehension to filter out stopwords:\n",
        "# For each word in tokens, if its lowercase form is not in stop_words, keep it.\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_tokens)  # Output the filtered list of tokens (stopwords removed)\n"
      ],
      "metadata": {
        "id": "fguNzzghHfof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b87809-8799-4056-8bd8-619730774360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['LIKE', 'IBM', 'SESSION']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "# Converting the words to their root form - Stemming process\n",
        "\n",
        "from nltk.stem import PorterStemmer  # Import the PorterStemmer from NLTK\n",
        "\n",
        "stemmer = PorterStemmer()  # Create a stemmer object\n",
        "words = [\"playing\", \"played\", \"plays\"]  # List of words to stem\n",
        "\n",
        "# List comprehension to stem each word in the list\n",
        "# For each 'word' in 'words', apply 'stemmer.stem(word)' and collect the result in a new list called 'stems'\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stems)  # Output: ['play', 'play', 'play']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkaVeFzcRBWq",
        "outputId": "ed6c2616-22c7-47ee-8e6d-f83ec48d6cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'play', 'play']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 LEMMATIZATION\n",
        "# Convert the words to their dictionary (base) form using lemmatization.\n",
        "# Example: \"Better\" --> \"good\", \"Running\" --> \"run\"\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer  # Import WordNetLemmatizer from NLTK\n",
        "import nltk  # Import the NLTK library\n",
        "\n",
        "nltk.download('wordnet')    # Download the WordNet data needed for lemmatization\n",
        "nltk.download('omw-1.4')    # Download the Open Multilingual Wordnet data (for better coverage)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()  # Create a lemmatizer object\n",
        "\n",
        "# Lemmatize the word \"better\" as an adjective ('a'), returns its dictionary form \"good\"\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "\n",
        "# Lemmatize the word \"running\" as a verb ('v'), returns its dictionary form \"run\"\n",
        "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY-Xm8nIRIdx",
        "outputId": "025e4b72-9a36-4b7c-a0b2-003f5b9f6fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 PART OF SPEECH TAGGING\n",
        "# This code identifies the part of speech (POS) for each word in a sentence.\n",
        "# Example: \"run\" --> VERB, \"apple\" --> NOUN, \"beautiful\" --> ADJECTIVE\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer  # Import tokenizer for splitting sentences into words\n",
        "import nltk  # Import NLTK library\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # Download POS tagging model\n",
        "from nltk import pos_tag  # Import POS tagger\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()  # Create a tokenizer object\n",
        "\n",
        "sentence = \"I like to play football\"  # Input sentence\n",
        "tokens = tokenizer.tokenize(sentence)  # Split the sentence into tokens (words)\n",
        "\n",
        "# Assign part of speech tags to each token\n",
        "# pos_tag returns a list of tuples: (word, POS tag)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(\"TOKENS :\", tokens)      # Output the list of tokens\n",
        "print(\"POS_TAGS :\", pos_tags)  # Output the list of (token, POS) pairs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpehsRZHU_GW",
        "outputId": "32ed5df8-7dc4-48d1-d833-52f9e978c7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKENS : ['I', 'like', 'to', 'play', 'football']\n",
            "POS_TAGS : [('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('play', 'VB'), ('football', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 NAMED ENTITY RECOGNITION (NER)\n",
        "# This code identifies and classifies named entities (like people, places, organizations) in text.\n",
        "# Example:\n",
        "#   \"I BOUGHT A APPLE COMPUTER\" --> 'APPLE' as ORGANISATION\n",
        "#   \"I BOUGHT AN APPLE FROM THE MARKET TO MAKE JUICE\" --> 'APPLE' as FRUIT\n",
        "\n",
        "import spacy  # Import the spaCy library for NLP\n",
        "\n",
        "# Load the small English model for spaCy, which includes NER capabilities\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input sentence for entity recognition\n",
        "sentence = nlp(\"Barack Obama was born in Hawaii.\")\n",
        "\n",
        "# Iterate over detected entities in the sentence\n",
        "for ent in sentence.ents:\n",
        "    # Print the entity text and its label (type)\n",
        "    print((ent.text, ent.label_))\n",
        "\n",
        "# Output:\n",
        "# ('Barack Obama', 'PERSON')\n",
        "# ('Hawaii', 'GPE')\n",
        "\n",
        "# Explanation:\n",
        "# - 'Barack Obama' is recognized as a PERSON.\n",
        "# - 'Hawaii' is recognized as a GPE (Geo-Political Entity, such as a country, state, or city).\n"
      ],
      "metadata": {
        "id": "HgWHh739GLy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31143c64-f663-4cd9-d922-327aa1623349"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Barack Obama', 'PERSON')\n",
            "('Hawaii', 'GPE')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. BAG OF WORDS\n",
        "# This code demonstrates the Bag of Words technique, which converts text into numerical feature vectors.\n",
        "# Each unique word in the corpus becomes a feature (a column), and each sentence/document is represented as a vector\n",
        "# indicating the presence (1) or absence (0) of each word.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Import CountVectorizer from scikit-learn\n",
        "\n",
        "texts = [\"I LIKE FOOTBALL\", \"FOOTBALL IS GREAT\"]  # Input sentences/documents\n",
        "\n",
        "vectorizer = CountVectorizer()  # Create a CountVectorizer object\n",
        "\n",
        "x = vectorizer.fit_transform(texts)  # Learn the vocabulary and transform the texts into vectors\n",
        "\n",
        "# Get the list of feature names (unique words found, sorted alphabetically)\n",
        "print(vectorizer.get_feature_names_out())  # Output: ['football' 'great' 'is' 'like']\n",
        "\n",
        "# Convert the sparse matrix to a dense array and print it\n",
        "print(x.toarray())  # Output: [[1 0 0 1] [1 1 1 0]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REPzX1EeOyRo",
        "outputId": "a60d3476-3068-44e0-bb3d-092d71dda8e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['football' 'great' 'is' 'like']\n",
            "[[1 0 0 1]\n",
            " [1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "# This code converts text data into a matrix of TF-IDF scores.\n",
        "# TF-IDF reflects how important a word is to a document in the context of the entire dataset.\n",
        "# Words that appear frequently in one document but not in others get higher scores.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer\n",
        "\n",
        "texts = [\"I love NLP\", \"NLP is great\"]  # Input sentences/documents\n",
        "\n",
        "tfidf = TfidfVectorizer()  # Create a TfidfVectorizer object\n",
        "\n",
        "x = tfidf.fit_transform(texts)  # Learn the vocabulary and transform the texts into TF-IDF vectors\n",
        "\n",
        "# Print the unique words (features) found in the data, sorted alphabetically\n",
        "print(tfidf.get_feature_names_out())  # Output: ['great' 'is' 'love' 'nlp']\n",
        "\n",
        "# Print the TF-IDF matrix for each sentence\n",
        "print(x.toarray())\n",
        "# Output:\n",
        "# [[0.         0.         0.81480247 0.57973867]\n",
        "#  [0.6316672  0.6316672  0.         0.44943642]]\n",
        "\n",
        "# Explanation:\n",
        "# Each value represents the importance of a word in a sentence.\n",
        "# For the first sentence (\"I love NLP\"):\n",
        "#   - 'love' and 'nlp' have higher values, meaning they are more important in this sentence.\n",
        "# For the second sentence (\"NLP is great\"):\n",
        "#   - 'great', 'is', and 'nlp' have nonzero values, showing their importance in this context.\n"
      ],
      "metadata": {
        "id": "McPr0CYXRXoW",
        "outputId": "60a92f07-0414-4fdc-f359-114ae5e3e53a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['great' 'is' 'love' 'nlp']\n",
            "[[0.         0.         0.81480247 0.57973867]\n",
            " [0.6316672  0.6316672  0.         0.44943642]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. WORD2VEC - Convert words into vectors (using pre-trained embeddings)\n",
        "# Word2Vec represents each word as a vector of numbers, capturing semantic meaning.\n",
        "# Example: king - man + woman â‰ˆ queen\n",
        "\n",
        "# If gensim is not installed, uncomment the next line:\n",
        "# !pip install gensim\n",
        "\n",
        "import gensim.downloader as api  # Import downloader for pre-trained models\n",
        "\n",
        "# Load the pre-trained Word2Vec model (Google News vectors)\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Find the word most similar to 'king' + 'woman' - 'man'\n",
        "similar = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n",
        "\n",
        "print(similar[:1])  # Output: [('queen', similarity_score)]\n"
      ],
      "metadata": {
        "id": "ufxs2EGaR1tP",
        "outputId": "e34b66bc-4333-4cb8-fb84-e83c63480cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-716419594>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# !pip install gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m  \u001b[0;31m# Import downloader for pre-trained models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the pre-trained Word2Vec model (Google News vectors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}