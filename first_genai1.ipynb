{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
    
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk  #natular language tool kit library offers prebuilt algori , datasets,and lot of module as below etc\n",
        "# Major Components of NLTK\n",
        "# Module\tDescription\n",
        "# nltk.tokenize\tSplits text into sentences or words.\n",
        "# nltk.corpus\tAccess to large collections of text (like stopwords, movie reviews, names, etc.).\n",
        "# nltk.stem\tPerforms stemming (reduces words to their root form).\n",
        "# nltk.tag\tPart-of-speech tagging for words.\n",
        "# nltk.chunk\tNamed entity recognition and phrase chunking.\n",
        "# nltk.parse\tParsing text using grammar rules.\n",
        "# nltk.classify\tTools for text classification.\n",
        "# nltk.metrics\tFunctions to evaluate NLP models.\n",
        "# nltk.probability\tLanguage modeling, frequency distributions.\n",
        "# nltk.sem\tTools for representing and processing meaning (semantics)."
      ],
      "metadata": {
        "id": "SSXG4JIBKdg3",
        "outputId": "a290720f-8b58-4d4a-b035-f53b220f84dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn #library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a02qizXLDOGO",
        "outputId": "8074d962-ea5e-436c-b994-537345a7076d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 TOKENIZATION\n",
        "# \"I LIKE IBM SESSION\" -> [\"I\", \"LIKE\", \"IBM\", \"SESSION\"]\n",
        "# Each word is referred to as Tokens - 4\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+') #\\w+ wont considee spaces, punctution etc , as  it didn consider  fullstop while tokenizing\n",
        "\n",
        "text = \"I LIKE IBM SESSION!\"\n",
        "print(text.split())\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "print(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7TsLQTlFR0J",
        "outputId": "db877819-c9a9-445c-ba97-689ee026bcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'LIKE', 'IBM', 'SESSION!']\n",
            "I LIKE IBM SESSION!\n",
            "['I', 'LIKE', 'IBM', 'SESSION']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 stopwords removal\n",
        "\n",
        "# The purpose of this code is to remove stopwords from a list of tokens.\n",
        "# Stopwords are common words (like \"the\", \"is\", \"in\") that are usually filtered out in NLP tasks.\n",
        "\n",
        "# Example input:  [Yesterday I went to shopping inorder to buy an Iphone]\n",
        "# Expected output: [Yesterday, went, shopping, inorder, buy, Iphone]\n",
        "\n",
        "from nltk.corpus import stopwords  # Import the stopwords list from NLTK\n",
        "import nltk  # Import the NLTK library\n",
        "\n",
        "nltk.download('stopwords')  # Download the stopwords data (only needs to be done once)\n",
        "\n",
        "tokens = ['I', 'LIKE', 'IBM', 'SESSION']  # List of words (tokens) to process\n",
        "stop_words = set(stopwords.words('english'))  # Create a set of English stopwords\n",
        "\n",
        "# List comprehension to filter out stopwords:\n",
        "# For each word in tokens, if its lowercase form is not in stop_words, keep it.\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_tokens)  # Output the filtered list of tokens (stopwords removed)\n"
      ],
      "metadata": {
        "id": "fguNzzghHfof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b87809-8799-4056-8bd8-619730774360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['LIKE', 'IBM', 'SESSION']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "# Converting the words to their root form - Stemming process\n",
        "\n",
        "from nltk.stem import PorterStemmer  # Import the PorterStemmer from NLTK\n",
        "\n",
        "stemmer = PorterStemmer()  # Create a stemmer object\n",
        "words = [\"playing\", \"played\", \"plays\"]  # List of words to stem\n",
        "\n",
        "# List comprehension to stem each word in the list\n",
        "# For each 'word' in 'words', apply 'stemmer.stem(word)' and collect the result in a new list called 'stems'\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stems)  # Output: ['play', 'play', 'play']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkaVeFzcRBWq",
        "outputId": "ed6c2616-22c7-47ee-8e6d-f83ec48d6cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'play', 'play']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 LEMMATIZATION\n",
        "# Convert the words to their dictionary (base) form using lemmatization.\n",
        "# Example: \"Better\" --> \"good\", \"Running\" --> \"run\"\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer  # Import WordNetLemmatizer from NLTK\n",
        "import nltk  # Import the NLTK library\n",
        "\n",
        "nltk.download('wordnet')    # Download the WordNet data needed for lemmatization\n",
        "nltk.download('omw-1.4')    # Download the Open Multilingual Wordnet data (for better coverage)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()  # Create a lemmatizer object\n",
        "\n",
        "# Lemmatize the word \"better\" as an adjective ('a'), returns its dictionary form \"good\"\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "\n",
        "# Lemmatize the word \"running\" as a verb ('v'), returns its dictionary form \"run\"\n",
        "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY-Xm8nIRIdx",
        "outputId": "025e4b72-9a36-4b7c-a0b2-003f5b9f6fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 PART OF SPEECH TAGGING\n",
        "# This code identifies the part of speech (POS) for each word in a sentence.\n",
        "# Example: \"run\" --> VERB, \"apple\" --> NOUN, \"beautiful\" --> ADJECTIVE\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer  # Import tokenizer for splitting sentences into words\n",
        "import nltk  # Import NLTK library\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # Download POS tagging model\n",
        "from nltk import pos_tag  # Import POS tagger\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()  # Create a tokenizer object\n",
        "\n",
        "sentence = \"I like to play football\"  # Input sentence\n",
        "tokens = tokenizer.tokenize(sentence)  # Split the sentence into tokens (words)\n",
        "\n",
        "# Assign part of speech tags to each token\n",
        "# pos_tag returns a list of tuples: (word, POS tag)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(\"TOKENS :\", tokens)      # Output the list of tokens\n",
        "print(\"POS_TAGS :\", pos_tags)  # Output the list of (token, POS) pairs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpehsRZHU_GW",
        "outputId": "32ed5df8-7dc4-48d1-d833-52f9e978c7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKENS : ['I', 'like', 'to', 'play', 'football']\n",
            "POS_TAGS : [('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('play', 'VB'), ('football', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 NAMED ENTITY RECOGNITION (NER)\n",
        "# This code identifies and classifies named entities (like people, places, organizations) in text.\n",
        "# Example:\n",
        "#   \"I BOUGHT A APPLE COMPUTER\" --> 'APPLE' as ORGANISATION\n",
        "#   \"I BOUGHT AN APPLE FROM THE MARKET TO MAKE JUICE\" --> 'APPLE' as FRUIT\n",
        "\n",
        "import spacy  # Import the spaCy library for NLP\n",
        "\n",
        "# Load the small English model for spaCy, which includes NER capabilities\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input sentence for entity recognition\n",
        "sentence = nlp(\"Barack Obama was born in Hawaii.\")\n",
        "\n",
        "# Iterate over detected entities in the sentence\n",
        "for ent in sentence.ents:\n",
        "    # Print the entity text and its label (type)\n",
        "    print((ent.text, ent.label_))\n",
        "\n",
        "# Output:\n",
        "# ('Barack Obama', 'PERSON')\n",
        "# ('Hawaii', 'GPE')\n",
        "\n",
        "# Explanation:\n",
        "# - 'Barack Obama' is recognized as a PERSON.\n",
        "# - 'Hawaii' is recognized as a GPE (Geo-Political Entity, such as a country, state, or city).\n"
      ],
      "metadata": {
        "id": "HgWHh739GLy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31143c64-f663-4cd9-d922-327aa1623349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Barack Obama', 'PERSON')\n",
            "('Hawaii', 'GPE')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. BAG OF WORDS\n",
        "# This code demonstrates the Bag of Words technique, which converts text into numerical feature vectors.\n",
        "# Each unique word in the corpus becomes a feature (a column), and each sentence/document is represented as a vector\n",
        "# indicating the presence (1) or absence (0) of each word.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Import CountVectorizer from scikit-learn\n",
        "\n",
        "texts = [\"I LIKE FOOTBALL\", \"FOOTBALL IS GREAT\"]  # Input sentences/documents\n",
        "\n",
        "vectorizer = CountVectorizer()  # Create a CountVectorizer object\n",
        "\n",
        "x = vectorizer.fit_transform(texts)  # Learn the vocabulary and transform the texts into vectors\n",
        "\n",
        "# Get the list of feature names (unique words found, sorted alphabetically)\n",
        "print(vectorizer.get_feature_names_out())  # Output: ['football' 'great' 'is' 'like']\n",
        "\n",
        "# Convert the sparse matrix to a dense array and print it\n",
        "print(x.toarray())  # Output: [[1 0 0 1] [1 1 1 0]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REPzX1EeOyRo",
        "outputId": "a60d3476-3068-44e0-bb3d-092d71dda8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['football' 'great' 'is' 'like']\n",
            "[[1 0 0 1]\n",
            " [1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "# This code converts text data into a matrix of TF-IDF scores.\n",
        "# TF-IDF reflects how important a word is to a document in the context of the entire dataset.\n",
        "# Words that appear frequently in one document but not in others get higher scores.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer\n",
        "\n",
        "texts = [\"I love NLP\", \"NLP is great\"]  # Input sentences/documents\n",
        "\n",
        "tfidf = TfidfVectorizer()  # Create a TfidfVectorizer object\n",
        "\n",
        "x = tfidf.fit_transform(texts)  # Learn the vocabulary and transform the texts into TF-IDF vectors\n",
        "\n",
        "# Print the unique words (features) found in the data, sorted alphabetically\n",
        "print(tfidf.get_feature_names_out())  # Output: ['great' 'is' 'love' 'nlp']\n",
        "\n",
        "# Print the TF-IDF matrix for each sentence\n",
        "print(x.toarray())\n",
        "# Output:\n",
        "# [[0.         0.         0.81480247 0.57973867]\n",
        "#  [0.6316672  0.6316672  0.         0.44943642]]\n",
        "\n",
        "# Explanation:\n",
        "# Each value represents the importance of a word in a sentence.\n",
        "# For the first sentence (\"I love NLP\"):\n",
        "#   - 'love' and 'nlp' have higher values, meaning they are more important in this sentence.\n",
        "# For the second sentence (\"NLP is great\"):\n",
        "#   - 'great', 'is', and 'nlp' have nonzero values, showing their importance in this context.\n"
      ],
      "metadata": {
        "id": "McPr0CYXRXoW",
        "outputId": "60a92f07-0414-4fdc-f359-114ae5e3e53a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['great' 'is' 'love' 'nlp']\n",
            "[[0.         0.         0.81480247 0.57973867]\n",
            " [0.6316672  0.6316672  0.         0.44943642]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. WORD2VEC - Convert words into vectors (using pre-trained embeddings)\n",
        "# Word2Vec represents each word as a vector of numbers, capturing semantic meaning.\n",
        "# Example: king - man + woman ≈ queen\n",
        "\n",
        "# If gensim is not installed, uncomment the next line:\n",
        "# !pip install gensim\n",
        "\n",
        "import gensim.downloader as api  # Import downloader for pre-trained models\n",
        "\n",
        "# Load the pre-trained Word2Vec model (Google News vectors)\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Find the word most similar to 'king' + 'woman' - 'man'\n",
        "similar = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n",
        "\n",
        "print(similar[:1])  # Output: [('queen', similarity_score)]\n"
      ],
      "metadata": {
        "id": "ufxs2EGaR1tP",
        "outputId": "4a2a197f-eea0-4368-c839-083e67153407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-716419594>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# !pip install gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m  \u001b[0;31m# Import downloader for pre-trained models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the pre-trained Word2Vec model (Google News vectors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "#step1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k2LSvHviYHs",
        "outputId": "f475f238-f344-416c-eda2-ae5c9988aef4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "              Transformeres\n",
        "              "
      ],
      "metadata": {
        "id": "m9pQ1M7qfLVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "from transformers import AutoTokenizer  # Import the AutoTokenizer class from Hugging Face Transformers\n",
        "\n",
        "# Load the pre-trained GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "59ff35cb7b4b42cda54bff5167894f4d",
            "dbf7c4cb39e14d199d90bed9fabec458",
            "dbe7a4151d3843868d02733b31345c14",
            "6ceb945fd07149f99696a5315bae3518",
            "1bebde48f0ca4c3b9975a1b45b846af8",
            "09b89ff209c14070b1ee55ae21e0c88e",
            "135baa1ad7474004bd1e6b7be1365fc1",
            "d1843990c16a49cdb27ffc14a6d6e52b",
            "9c381f7bc4774eeea18806d888db5fd7",
            "6116a5e7407e48b6a072c1b4d1a4e0b5",
            "b8831f166d3d45448e815f56a7f4a935",
            "61db3414b938462fa4cfd9886c6ead77",
            "b54437a9fc044c2eba9e06a8469334c1",
            "d3071205d75f4e129208cad805571b58",
            "7f4ea5ca444f49e9b0d7c15ed5c63064",
            "5d519e3d4020491fb9be7512e3b10034",
            "8ead28429ce747a3ae7cdae5eea41982",
            "72c453a5ade942e5a5878156a9b30e53",
            "516e20e36ea24f05a123a7eaf335a601",
            "a4d20ae02ad44d0ea8d951974165a265",
            "9873d463402d49758ae1d93d594eb9c7",
            "2056f64eb9c34a9b9a10d67f84dcd86d",
            "c42b549adc744fd89461ae2e2bddc11f",
            "8a8a6b1e45e84d1da834377c46a4237e",
            "a1cf68f80c2c473ca521802d1e6f089d",
            "94345dfe98564e948fc6a0fd4f1ec905",
            "722ddd0a22304d0db9e5c8a015ee149f",
            "7d5ad864fc324e88874abeb537c0792b",
            "d5bc64e5d8d1456a86706e38a4ed4893",
            "1bd6ab88354d476a827798b7af496c14",
            "c3a185e27ea14e6d9ba506acd5405d92",
            "89a7fdafb5ac4000a711221bc85d2de2",
            "c3474fba8074411e81a40b4994516a93",
            "eaca777e79e94de99c3d5452673c2a5c",
            "98ee8fdb8b614b8db491aeb723ebc09a",
            "31c888b2204a474d88b28ae726cf80fd",
            "b4a184ca2d7a401eb402db9b697d6035",
            "b8da05ae278744cabbc988efa0b7d703",
            "dca2dfdc25314e0ab94684726fc56a87",
            "2e1187a42eb74fda9311466cd4c05eb6",
            "5936c35245ec47d286d20dd6bbc7aad8",
            "4c544aab567148edbb00053fd29444ba",
            "25c4cdcafc1b475c92f33d53c4f34391",
            "6803ac1c1bcb4ccf99874507d6b628da",
            "4438a71b6aa340d285f2b059814d5255",
            "7c73567436734ad1be8474ba6f48c556",
            "2c8ae945fce84386a73700c634a86f98",
            "6c40f9cea45543e39f53e174e724e9ef",
            "e8765552512a4be4a4ebce0c7c59c838",
            "bf2c7cddf3a847e0905713f6b6afd551",
            "adf9edc9a28640d09f6a38fd5e5ee680",
            "a62efe10a7194f88a9a975b58fcaa30f",
            "b6473fa01dab4316aae95343db3b93fa",
            "06a3ffddbc72428285b848572b6785f5",
            "be473d54fa1a466da24b67c267342dcf"
          ]
        },
        "id": "ZS1y7yABfjLh",
        "outputId": "0998d54f-6047-40a8-83f3-98c12c784129"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59ff35cb7b4b42cda54bff5167894f4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61db3414b938462fa4cfd9886c6ead77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c42b549adc744fd89461ae2e2bddc11f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaca777e79e94de99c3d5452673c2a5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4438a71b6aa340d285f2b059814d5255"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. TOKENIZATION USING TRANSFORMERS (GPT-2)\n",
        "# This code demonstrates how to tokenize a sentence using a pre-trained transformer tokenizer (GPT-2).\n",
        "# Each word or subword in the input is converted to a unique token ID used by the model.\n",
        "# You can also decode each token ID back to its text representation.\n",
        "\n",
        "from transformers import AutoTokenizer  # Import the AutoTokenizer class from Hugging Face Transformers\n",
        "\n",
        "# Load the pre-trained GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize the input sentence and get the token IDs as PyTorch tensors\n",
        "ids = tokenizer(\"It was a dark and stormy\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# For each token ID in the first (and only) sentence\n",
        "for t in ids[0]:\n",
        "    # Print the token ID, a tab, and the decoded text for that token\n",
        "    print(t, \"\\t:\", tokenizer.decode(t))\n",
        "\n",
        "# Explanation:\n",
        "# - The input sentence is split into tokens as understood by the GPT-2 model.\n",
        "# - Each token is mapped to a unique integer ID (used internally by the model).\n",
        "# - Some words (like \"stormy\") may be split into multiple tokens (\"storm\", \"y\") due to the subword tokenization approach.\n",
        "# - Decoding each token ID gives the corresponding text fragment.\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhWT7e4Cf6wE",
        "outputId": "fe4346c1-9164-4979-f64a-195c93ef0ef8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1026) \t: It\n",
            "tensor(373) \t:  was\n",
            "tensor(257) \t:  a\n",
            "tensor(3223) \t:  dark\n",
            "tensor(290) \t:  and\n",
            "tensor(6388) \t:  storm\n",
            "tensor(88) \t: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMNRZNEAkdU9",
        "outputId": "a88747a5-619e-495f-f1a7-5029cfeaad7e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. LOADING THE GPT-2 MODEL AND PROCESSING INPUT\n",
        "# This code loads the pre-trained GPT-2 language model and processes your tokenized input.\n",
        "# It demonstrates how the model handles input sequences and produces output logits (prediction scores).\n",
        "\n",
        "from transformers import AutoModelForCausalLM  # Import the GPT-2 model class\n",
        "\n",
        "# Load the pre-trained GPT-2 model\n",
        "# pad_token_id=tokenizer.eos_token_id sets the padding token to the end-of-sequence token\n",
        "# This is needed because GPT-2 doesn't have a default padding token\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Pass the tokenized input (ids) to the model to get the outputs\n",
        "outputs = gpt2(ids)\n",
        "\n",
        "# Print the shape of the output logits\n",
        "print(outputs.logits.shape)  # Output: torch.Size([1, sequence_length, 50257])\n",
        "\n",
        "# Explanation of padding with example:\n",
        "# If you have sentences of different lengths in a batch:\n",
        "# - \"I like NLP\" → 3 tokens\n",
        "# - \"I like to play video games\" → 6 tokens\n",
        "#\n",
        "# The model needs all inputs to be the same length, so padding is added:\n",
        "# - \"I like NLP [PAD] [PAD] [PAD]\" → 6 tokens (padded to match longest)\n",
        "# - \"I like to play video games\" → 6 tokens (no padding needed)\n",
        "#\n",
        "# This allows the model to process multiple sentences together efficiently.\n",
        "\n",
        "# What the output means:\n",
        "# outputs.logits.shape = [batch_size, sequence_length, vocab_size]\n",
        "# - batch_size: number of input sequences (1 in this case)\n",
        "# - sequence_length: number of tokens in your input sentence\n",
        "# - vocab_size: size of the model's vocabulary (50,257 for GPT-2)\n",
        "#\n",
        "# Each value in logits represents the model's confidence score for each possible token\n",
        "# at each position in the sequence.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "88e05dd8a79447019a186d2af9d65f82",
            "70fd7ed4deab43679e90ed2cc694932d",
            "e39adfc291124f9eaaa5ca424d6a82d1",
            "44bce22930174b31be024ccea221365d",
            "09f822656c674a99918393cd79c7dc37",
            "e0fa761ac2ab40cfab7d00f0f6607678",
            "92d2f9823ee44981ba86483776ea306f",
            "88a8a02795274cdeab2b1edef2565111",
            "c34d906cdee643a7906c426094179723",
            "70f4ef46867440c3b0983b338e4cd85a",
            "2c17f312fbcc4f379112ccbaa9f1c35f",
            "0707e8b328cf41378d3d92f36deb6460",
            "9c8ae63951d54824b1b5a06a1a66118b",
            "b356a923454b420b8d3c518f55758d32",
            "ffb1e99700bc4caf9525a6e52b26da10",
            "6a0e2f665d9b489abf70ea77d08c84fd",
            "266870283bc5470d8f63a2ac8b51012c",
            "7d2ab9cff0b74211be52ce8c0046e641",
            "f6439f0f6a8545508cd86d8748aac0fd",
            "c7c3fb47aade4f6484d5b8760d73b8b0",
            "8a050e25dc104e3f9ec490c177ec84eb",
            "347cf5b27c7848feacb2a4e8603a9e1b"
          ]
        },
        "id": "_1q2TMoZkgSM",
        "outputId": "321d9a34-790c-4548-ab28-67be346f66c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88e05dd8a79447019a186d2af9d65f82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0707e8b328cf41378d3d92f36deb6460"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 7, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. PREDICT THE NEXT WORDS USING GPT-2 GENERATE FUNCTION\n",
        "# This code uses the GPT-2 model to generate text continuation for your input sentence.\n",
        "# The model predicts and appends new tokens (words/subwords) to the original input.\n",
        "\n",
        "# Generate text: the model will predict up to 20 new tokens after your input\n",
        "output_ids = gpt2.generate(ids, max_new_tokens=20)\n",
        "\n",
        "# Print the generated token IDs\n",
        "print(output_ids)\n",
        "\n",
        "# Decode the generated token IDs back into readable text\n",
        "print(tokenizer.decode(output_ids[0]))\n",
        "\n",
        "# Explanation:\n",
        "# - gpt2.generate(ids, max_new_tokens=20) takes your input tokens and generates up to 20 more tokens as a continuation.\n",
        "# - output_ids contains both your original input and the newly generated tokens.\n",
        "# - tokenizer.decode(output_ids[0]) converts the sequence of token IDs back into a human-readable sentence.\n",
        "# - The model continues your story, creating new sentences or phrases that follow your prompt.\n",
        "#\n",
        "# Example output:\n",
        "# 'It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the'\n",
        "#\n",
        "# This shows how GPT-2 can be used to generate creative text based on your input.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFQI68bIled2",
        "outputId": "fee501fc-ecb2-4902-81e8-8f239285a94b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1026,   373,   257,  3223,   290,  6388,    88,  1755,    13,   383,\n",
            "          2344,   373, 19280,    11,   290,   262, 15114,   547,  7463,    13,\n",
            "           383,  2344,   373, 19280,    11,   290,   262]])\n",
            "It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the\n"
          ]
        }
      ]
    }
  ]
}