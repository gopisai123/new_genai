{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
    {
      "cell_type": "code",
      "source": [
        "#3. TOKENIZATION USING TRANSFORMERS (GPT-2)\n",
        "# This code demonstrates how to tokenize a sentence using a pre-trained transformer tokenizer (GPT-2).\n",
        "# Each word or subword in the input is converted to a unique token ID used by the model.\n",
        "# You can also decode each token ID back to its text representation.\n",
        "\n",
        "from transformers import AutoTokenizer  # Import the AutoTokenizer class from Hugging Face Transformers\n",
        "\n",
        "# Load the pre-trained GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize the input sentence and get the token IDs as PyTorch tensors\n",
        "ids = tokenizer(\"It was a dark and stormy\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# For each token ID in the first (and only) sentence\n",
        "for t in ids[0]:\n",
        "    # Print the token ID, a tab, and the decoded text for that token\n",
        "    print(t, \"\\t:\", tokenizer.decode(t))\n",
        "\n",
        "# Explanation:\n",
        "# - The input sentence is split into tokens as understood by the GPT-2 model.\n",
        "# - Each token is mapped to a unique integer ID (used internally by the model).\n",
        "# - Some words (like \"stormy\") may be split into multiple tokens (\"storm\", \"y\") due to the subword tokenization approach.\n",
        "# - Decoding each token ID gives the corresponding text fragment.\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhWT7e4Cf6wE",
        "outputId": "fe4346c1-9164-4979-f64a-195c93ef0ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1026) \t: It\n",
            "tensor(373) \t:  was\n",
            "tensor(257) \t:  a\n",
            "tensor(3223) \t:  dark\n",
            "tensor(290) \t:  and\n",
            "tensor(6388) \t:  storm\n",
            "tensor(88) \t: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMNRZNEAkdU9",
        "outputId": "a88747a5-619e-495f-f1a7-5029cfeaad7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. LOADING THE GPT-2 MODEL AND PROCESSING INPUT\n",
        "# This code loads the pre-trained GPT-2 language model and processes your tokenized input.\n",
        "# It demonstrates how the model handles input sequences and produces output logits (prediction scores).\n",
        "\n",
        "from transformers import AutoModelForCausalLM  # Import the GPT-2 model class\n",
        "\n",
        "# Load the pre-trained GPT-2 model\n",
        "# pad_token_id=tokenizer.eos_token_id sets the padding token to the end-of-sequence token\n",
        "# This is needed because GPT-2 doesn't have a default padding token\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Pass the tokenized input (ids) to the model to get the outputs\n",
        "outputs = gpt2(ids)\n",
        "\n",
        "# Print the shape of the output logits\n",
        "print(outputs.logits.shape)  # Output: torch.Size([1, sequence_length, 50257])\n",
        "\n",
        "# Explanation of padding with example:\n",
        "# If you have sentences of different lengths in a batch:\n",
        "# - \"I like NLP\" → 3 tokens\n",
        "# - \"I like to play video games\" → 6 tokens\n",
        "#\n",
        "# The model needs all inputs to be the same length, so padding is added:\n",
        "# - \"I like NLP [PAD] [PAD] [PAD]\" → 6 tokens (padded to match longest)\n",
        "# - \"I like to play video games\" → 6 tokens (no padding needed)\n",
        "#\n",
        "# This allows the model to process multiple sentences together efficiently.\n",
        "\n",
        "# What the output means:\n",
        "# outputs.logits.shape = [batch_size, sequence_length, vocab_size]\n",
        "# - batch_size: number of input sequences (1 in this case)\n",
        "# - sequence_length: number of tokens in your input sentence\n",
        "# - vocab_size: size of the model's vocabulary (50,257 for GPT-2)\n",
        "#\n",
        "# Each value in logits represents the model's confidence score for each possible token\n",
        "# at each position in the sequence.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "88e05dd8a79447019a186d2af9d65f82",
            "70fd7ed4deab43679e90ed2cc694932d",
            "e39adfc291124f9eaaa5ca424d6a82d1",
            "44bce22930174b31be024ccea221365d",
            "09f822656c674a99918393cd79c7dc37",
            "e0fa761ac2ab40cfab7d00f0f6607678",
            "92d2f9823ee44981ba86483776ea306f",
            "88a8a02795274cdeab2b1edef2565111",
            "c34d906cdee643a7906c426094179723",
            "70f4ef46867440c3b0983b338e4cd85a",
            "2c17f312fbcc4f379112ccbaa9f1c35f",
            "0707e8b328cf41378d3d92f36deb6460",
            "9c8ae63951d54824b1b5a06a1a66118b",
            "b356a923454b420b8d3c518f55758d32",
            "ffb1e99700bc4caf9525a6e52b26da10",
            "6a0e2f665d9b489abf70ea77d08c84fd",
            "266870283bc5470d8f63a2ac8b51012c",
            "7d2ab9cff0b74211be52ce8c0046e641",
            "f6439f0f6a8545508cd86d8748aac0fd",
            "c7c3fb47aade4f6484d5b8760d73b8b0",
            "8a050e25dc104e3f9ec490c177ec84eb",
            "347cf5b27c7848feacb2a4e8603a9e1b"
          ]
        },
        "id": "_1q2TMoZkgSM",
        "outputId": "321d9a34-790c-4548-ab28-67be346f66c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88e05dd8a79447019a186d2af9d65f82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0707e8b328cf41378d3d92f36deb6460"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 7, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. PREDICT THE NEXT WORDS USING GPT-2 GENERATE FUNCTION\n",
        "# This code uses the GPT-2 model to generate text continuation for your input sentence.\n",
        "# The model predicts and appends new tokens (words/subwords) to the original input.\n",
        "\n",
        "# Generate text: the model will predict up to 20 new tokens after your input\n",
        "output_ids = gpt2.generate(ids, max_new_tokens=20)\n",
        "\n",
        "# Print the generated token IDs\n",
        "print(output_ids)\n",
        "\n",
        "# Decode the generated token IDs back into readable text\n",
        "print(tokenizer.decode(output_ids[0]))\n",
        "\n",
        "# Explanation:\n",
        "# - gpt2.generate(ids, max_new_tokens=20) takes your input tokens and generates up to 20 more tokens as a continuation.\n",
        "# - output_ids contains both your original input and the newly generated tokens.\n",
        "# - tokenizer.decode(output_ids[0]) converts the sequence of token IDs back into a human-readable sentence.\n",
        "# - The model continues your story, creating new sentences or phrases that follow your prompt.\n",
        "#\n",
        "# Example output:\n",
        "# 'It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the'\n",
        "#\n",
        "# This shows how GPT-2 can be used to generate creative text based on your input.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFQI68bIled2",
        "outputId": "fee501fc-ecb2-4902-81e8-8f239285a94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1026,   373,   257,  3223,   290,  6388,    88,  1755,    13,   383,\n",
            "          2344,   373, 19280,    11,   290,   262, 15114,   547,  7463,    13,\n",
            "           383,  2344,   373, 19280,    11,   290,   262]])\n",
            "It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. SAMPLING TECHNIQUES FOR TEXT GENERATION WITH GPT-2\n",
        "# This code demonstrates how to use advanced sampling techniques to control randomness and creativity\n",
        "# when generating text with GPT-2. You can adjust parameters like top_k, top_p, and temperature.\n",
        "\n",
        "# do_sample=True enables random sampling (not just greedy/argmax selection)\n",
        "# top_k=100 limits sampling to the 100 most likely next tokens\n",
        "# top_p=0.9 enables nucleus (top-p) sampling, picking from the smallest set of tokens whose probabilities sum to 0.9\n",
        "# temperature=0.7 controls randomness (lower = more conservative, higher = more random)\n",
        "\n",
        "output_ids = gpt2.generate(\n",
        "    ids,                  # input token IDs (from your prompt)\n",
        "    max_new_tokens=30,    # generate up to 30 new tokens\n",
        "    do_sample=True,       # enable sampling for creative output\n",
        "    top_k=100,            # sample only from the top 100 most likely tokens\n",
        "    top_p=0.9,            # or from the smallest set with cumulative probability >= 0.9\n",
        "    temperature=0.7       # control randomness (0.7 is moderately creative)\n",
        ")\n",
        "\n",
        "# Decode the generated token IDs back into readable text\n",
        "generated_text = tokenizer.decode(output_ids[0])\n",
        "print(generated_text)\n",
        "\n",
        "# Explanation:\n",
        "# - The model takes your prompt and generates a continuation using random sampling.\n",
        "# - top_k and top_p limit the pool of possible next words, balancing creativity and coherence.\n",
        "# - temperature adjusts how \"bold\" or \"safe\" the model is when picking the next word.\n",
        "# - The output is a more varied and natural-sounding text, not just the most predictable completion.\n",
        "\n",
        "# Example output:\n",
        "# 'It was a dark and stormy and scary night,\" she said, \"but it was also really nice being inside. We couldn’t think of any other way to go.\"'\n"
      ],
      "metadata": {
        "id": "eXvx6ZIAm801",
        "outputId": "d8bbbe37-7e2b-409c-d5fa-7e17b673af75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gpt2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2119195589>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# temperature=0.7 controls randomness (lower = more conservative, higher = more random)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m output_ids = gpt2.generate(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# input token IDs (from your prompt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# generate up to 30 new tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gpt2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. SAMPLING TECHNIQUES WITH GPT-2\n",
        "# Fixes the NameError by properly loading the model and tokenizer.\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel  # Import the tokenizer and model classes\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize the input text to get `ids`\n",
        "input_text = \"It was a dark and stormy night\"\n",
        "ids = tokenizer.encode(input_text, return_tensors=\"pt\")  # Creates tensor input_ids\n",
        "\n",
        "# Generate text with sampling\n",
        "output_ids = gpt2.generate(\n",
        "    ids,                  # Input token IDs\n",
        "    max_new_tokens=30,\n",
        "    do_sample=True,\n",
        "    top_k=100,\n",
        "    top_p=0.9,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "1hq7yhznrTfo",
        "outputId": "a47827ad-cbee-433e-852a-ed7914520735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "d25504e006204bd5b118d80a00584f8a",
            "732dce95b0254509a7b28dafa6363a87",
            "d1ee64a7f8af413baa323b7bcf17029d",
            "e1f2e264057b434f98f6b13623b3baf6",
            "8072cde3afec4da98b8d2ea6f586dc69",
            "d95ab4074133462793f5b38e37af487f",
            "80bab7f6f2cb4ffa8ce6d1eeeded8b47",
            "c976fa6d81534f4db471c27856cec15c",
            "72489deadcd5473aae45f05ae3fd3b65",
            "70ca4d851b13485c9e719631d5fbea22",
            "1df132441a0547628242f8a8b1a52e8d",
            "e421bf794bf243068440f9e80d01f56b",
            "4a61541cf0f040ca9940eef0c8540f98",
            "f0a6c22e63ed4cd3ad833330ae056106",
            "6b0e08c4bff64ce7a98cfd2976b3bc44",
            "44493b23fe4f48749d013fefd09d8d38",
            "2d7a6b238c6649b195cc2e4d5fd4e2bd",
            "7314f0e4220349ea88240aaf821661b4",
            "3d6fc3f197244d058f02129bd2dd8507",
            "24093dfaa45a44cea49a1620537ad7f0",
            "fdba5f0a3e5a46eda7639aabd7bdbc7e",
            "3792a052d2b842459931cc62c8a83cab",
            "0493d0edaf12464abb6ed82ac3a3cab5",
            "d2aa8cfd45774da6b81f3a5557f6cd0a",
            "d945ddfb35364116899e11cdeb9f8120",
            "6373e80abcc04f42bd4e9444723ae6e2",
            "a9bd89892059472fbccbedeb10a0414a",
            "b8e48f0a8cc94e91ab1db0691e10b62d",
            "5c409b47b48f47579ab745b8515221d8",
            "add8fa56c4274a1693ea170e213b51cd",
            "6bcac97d19a648669c3cc6382a8ddf1f",
            "d12eb9022de04e78ab56b200c905ac6d",
            "a9e759c7c7bc4ac1b90978d407902a01",
            "3e07c54a3441442d9f9ed23909caeb75",
            "f169ee591d9c455caa12e3dd58e8cb3c",
            "c243e9f1da8d4ec098a3e2a59c96386f",
            "677cfe62c9d741ebb62a1a28397bba28",
            "c1ceaa6b02ee42798be3884f4d585585",
            "1050a12b66c74fd48fd90758145ca865",
            "b8704a67bbf8475a8c81ca438e5e75fa",
            "92426b7a00664740b3c80800f7ce1d96",
            "f99fcc0f0db64bbe80ec80a56f11e54f",
            "04510cc12e2845a7b109b14ab214103b",
            "478ca7316f1d47da9d9bac92bb66b0a0",
            "484fff14ce6f47ea981fc202853d6121",
            "88323f1ae37b4610a4119fd1e296c942",
            "ad100b8fe8d746fe9831b847f784ea7f",
            "ae72204a716142e5a0ab16f3f5d848be",
            "306eff9ae7074e07b3cad002d45c2eb7",
            "af95dc1e329147e39357816f4f2619bc",
            "63a436a052eb4ee28350cea3ed119d39",
            "12868c4fabdb40cfa27f80e3e46039c1",
            "90ca800178574082ba54ff7a93b96cff",
            "6e5ee5e9ceaf47549adb55858c723f40",
            "bec5254097d64a8e9f66a4256084bd92",
            "439310fc7be04417bd4f9460807b800d",
            "56f2adee7f5c403cad70bfa780d4331f",
            "1a245ad2db7b4f338d585bc8674fff51",
            "348bb125b44649c68ad76014e64610be",
            "b36d4da0df4a42bd8481e39a9bab5132",
            "5af0825fe65a48599ee3fb9e3b6b2cf7",
            "fe67c0b0f72a46369346b9924f6d9188",
            "2a5ddb466c1543f69dcedc50e5583e2d",
            "b855f6d965b645c0953addeac906c993",
            "31870db6b36448edabcb22e2e5af4ae8",
            "d9fde576a1f8469085f3e93871a4f033",
            "84887ad6c0234470a520c98fcd5a55f0",
            "9c52c6b146974ec4946e292c2fbb5932",
            "1ccf918874fe4af197dacc3da063f0da",
            "741db397165a48e5b5bce50dadb1857d",
            "8aee456880054292a3d08bf39ea52b60",
            "0738dab9227a4eee850a361f30b6e8b6",
            "e6b870473824491a99a5fbb0c7250059",
            "37e1ca62744a40399e0ce97a8ef5b7fd",
            "f69b70b808d543eaa769ce2263d7a24b",
            "bc78409151894278bef73144bb59c109",
            "c81b93f1c17245f7b92a5a6cd0e3765b"
          ]
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d25504e006204bd5b118d80a00584f8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e421bf794bf243068440f9e80d01f56b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0493d0edaf12464abb6ed82ac3a3cab5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e07c54a3441442d9f9ed23909caeb75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "484fff14ce6f47ea981fc202853d6121"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "439310fc7be04417bd4f9460807b800d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84887ad6c0234470a520c98fcd5a55f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night and the trees were dying down. I could see the trees and it was raining heavily. I heard the sound of a car being pulled over and then\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. MANUAL PREDICTION USING LOGITS (EXPLAINED)\n",
        "# This code gets the model's prediction scores (logits) for the next word after your input.\n",
        "\n",
        "final_logits = gpt2(ids).logits[0][-1]  #\n",
        "# Explanation:\n",
        "# - gpt2(ids).logits returns a tensor of shape [batch_size, sequence_length, vocab_size]\n",
        "# - [0] selects the first (and only) sentence in the batch\n",
        "# - [-1] selects the logits for the last token in your input (i.e., for predicting the next word)\n",
        "# So, final_logits contains the scores for all possible next tokens after your input.\n",
        "\n",
        "final_logits.argmax()\n",
        "# This finds the token ID with the highest score (the model's top prediction for the next word).\n",
        "\n",
        "# Example output:\n",
        "# tensor(37259)\n",
        "# This means the most likely next token, according to the model, has the ID 37259.\n"
      ],
      "metadata": {
        "id": "Wc3P2uMlZgXV",
        "outputId": "bdc54ed7-699a-4250-c2ee-32eec5a471be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(13)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}
